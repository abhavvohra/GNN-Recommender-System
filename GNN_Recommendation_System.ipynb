{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN-based Recommendation System\n",
    "\n",
    "This notebook implements a Graph Neural Network (GNN) based recommender system using the MovieLens 100K dataset. The system models user-movie interactions as a bipartite graph and uses message passing with graph convolutions to capture complex relationships.\n",
    "\n",
    "## Target Performance\n",
    "- **RMSE**: ‚â§ 0.90\n",
    "- **MAE**: ‚â§ 0.72\n",
    "- **Precision@K**: ‚â• 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Import custom modules\n",
    "from data_loader import MovieLensDataLoader\n",
    "from gnn_model import GNNRecommender, CollaborativeFilteringLoss\n",
    "from trainer import RecommenderTrainer\n",
    "from evaluator import RecommenderEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading MovieLens 100K dataset...\")\n",
    "data_loader = MovieLensDataLoader()\n",
    "ratings_df = data_loader.load_ratings()\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Total ratings: {len(ratings_df):,}\")\n",
    "print(f\"Number of users: {len(data_loader.user_encoder.classes_):,}\")\n",
    "print(f\"Number of items: {len(data_loader.item_encoder.classes_):,}\")\n",
    "print(f\"Sparsity: {(1 - len(ratings_df) / (len(data_loader.user_encoder.classes_) * len(data_loader.item_encoder.classes_))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset statistics\n",
    "print(\"\\n=== Dataset Statistics ===\")\n",
    "print(ratings_df.describe())\n",
    "print(\"\\n=== First 5 rows ===\")\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Rating distribution\n",
    "axes[0, 0].hist(ratings_df['rating'], bins=5, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Rating Distribution')\n",
    "axes[0, 0].set_xlabel('Rating')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# User activity distribution\n",
    "user_counts = ratings_df['user_id'].value_counts()\n",
    "axes[0, 1].hist(user_counts, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('User Activity Distribution')\n",
    "axes[0, 1].set_xlabel('Number of Ratings per User')\n",
    "axes[0, 1].set_ylabel('Number of Users')\n",
    "\n",
    "# Item popularity distribution\n",
    "item_counts = ratings_df['item_id'].value_counts()\n",
    "axes[1, 0].hist(item_counts, bins=50, alpha=0.7, color='salmon', edgecolor='black')\n",
    "axes[1, 0].set_title('Item Popularity Distribution')\n",
    "axes[1, 0].set_xlabel('Number of Ratings per Item')\n",
    "axes[1, 0].set_ylabel('Number of Items')\n",
    "\n",
    "# Rating vs timestamp\n",
    "sample_df = ratings_df.sample(5000)  # Sample for visualization\n",
    "axes[1, 1].scatter(sample_df['timestamp'], sample_df['rating'], alpha=0.5, color='purple')\n",
    "axes[1, 1].set_title('Ratings over Time (Sample)')\n",
    "axes[1, 1].set_xlabel('Timestamp')\n",
    "axes[1, 1].set_ylabel('Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Construction and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bipartite graph\n",
    "print(\"Creating bipartite graph...\")\n",
    "graph_data, num_users, num_items = data_loader.create_bipartite_graph(ratings_df)\n",
    "\n",
    "print(f\"Graph created successfully!\")\n",
    "print(f\"Number of nodes: {graph_data.x.shape[0]:,}\")\n",
    "print(f\"Number of edges: {graph_data.edge_index.shape[1]:,}\")\n",
    "print(f\"Node feature dimensions: {graph_data.x.shape[1]}\")\n",
    "print(f\"User nodes: {num_users:,}\")\n",
    "print(f\"Item nodes: {num_items:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze graph properties\n",
    "edge_weights = graph_data.edge_attr\n",
    "user_degrees = torch.bincount(graph_data.edge_index[0][graph_data.edge_index[0] < num_users])\n",
    "item_degrees = torch.bincount(graph_data.edge_index[0][graph_data.edge_index[0] >= num_users] - num_users)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Edge weight distribution\n",
    "axes[0, 0].hist(edge_weights.numpy(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_title('Edge Weight Distribution')\n",
    "axes[0, 0].set_xlabel('Edge Weight (Rating)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# User degree distribution\n",
    "axes[0, 1].hist(user_degrees.numpy(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].set_title('User Degree Distribution')\n",
    "axes[0, 1].set_xlabel('Degree')\n",
    "axes[0, 1].set_ylabel('Number of Users')\n",
    "\n",
    "# Item degree distribution\n",
    "axes[1, 0].hist(item_degrees.numpy(), bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[1, 0].set_title('Item Degree Distribution')\n",
    "axes[1, 0].set_xlabel('Degree')\n",
    "axes[1, 0].set_ylabel('Number of Items')\n",
    "\n",
    "# Node feature correlation heatmap\n",
    "user_features = graph_data.x[:num_users].numpy()\n",
    "corr_matrix = np.corrcoef(user_features.T)\n",
    "im = axes[1, 1].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1, 1].set_title('User Feature Correlation')\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Split data into train/validation/test sets\ndef prepare_data_splits(ratings_df, test_size=0.2, val_size=0.1, random_state=42):\n    \"\"\"Split ratings data into train, validation, and test sets.\"\"\"\n    train_val, test_df = train_test_split(ratings_df, test_size=test_size, random_state=random_state)\n    train_df, val_df = train_test_split(train_val, test_size=val_size/(1-test_size), random_state=random_state)\n    return train_df, val_df, test_df\n\ntrain_df, val_df, test_df = prepare_data_splits(ratings_df, test_size=0.2, val_size=0.1)\n\nprint(f\"Data split completed:\")\nprint(f\"Training set: {len(train_df):,} ratings ({len(train_df)/len(ratings_df)*100:.1f}%)\")\nprint(f\"Validation set: {len(val_df):,} ratings ({len(val_df)/len(ratings_df)*100:.1f}%)\")\nprint(f\"Test set: {len(test_df):,} ratings ({len(test_df)/len(ratings_df)*100:.1f}%)\")\n\n# Visualize rating distribution across splits\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, (df, name) in enumerate([(train_df, 'Train'), (val_df, 'Validation'), (test_df, 'Test')]):\n    axes[i].hist(df['rating'], bins=5, alpha=0.7, edgecolor='black')\n    axes[i].set_title(f'{name} Set Rating Distribution')\n    axes[i].set_xlabel('Rating')\n    axes[i].set_ylabel('Frequency')\n    axes[i].set_ylim(0, max([len(train_df[train_df['rating']==r]) for r in range(1,6)]) * 1.1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture and Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize model\nmodel = GNNRecommender(\n    num_users=num_users,\n    num_items=num_items,\n    feature_dim=graph_data.x.shape[1],\n    embedding_dim=64,\n    hidden_dim=128,\n    num_layers=3\n)\n\nprint(\"Model Architecture:\")\nprint(model)\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Training with Detailed Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Custom training function with detailed monitoring\nimport time\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm.notebook import tqdm\n\ndef train_with_monitoring(model, data, train_df, val_df, epochs=50, lr=0.001, batch_size=1024):\n    # Prepare data loaders\n    train_users = torch.LongTensor(train_df['user_id'].values)\n    train_items = torch.LongTensor(train_df['item_id'].values)\n    train_ratings = torch.FloatTensor(train_df['rating'].values)\n    \n    val_users = torch.LongTensor(val_df['user_id'].values)\n    val_items = torch.LongTensor(val_df['item_id'].values)\n    val_ratings = torch.FloatTensor(val_df['rating'].values)\n    \n    train_dataset = TensorDataset(train_users, train_items, train_ratings)\n    val_dataset = TensorDataset(val_users, val_items, val_ratings)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Setup training\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n    criterion = CollaborativeFilteringLoss(lambda_reg=0.01)\n    \n    model = model.to(device)\n    data = data.to(device)\n    \n    # Tracking metrics\n    metrics = {\n        'train_loss': [],\n        'val_loss': [],\n        'val_rmse': [],\n        'val_mae': [],\n        'learning_rate': [],\n        'epoch_time': [],\n        'gradient_norm': []\n    }\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    patience = 15\n    \n    print(\"Starting training with detailed monitoring...\")\n    \n    for epoch in range(epochs):\n        epoch_start_time = time.time()\n        \n        # Training phase\n        model.train()\n        total_train_loss = 0\n        total_grad_norm = 0\n        \n        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n        for batch_users, batch_items, batch_ratings in train_pbar:\n            batch_users = batch_users.to(device)\n            batch_items = batch_items.to(device)\n            batch_ratings = batch_ratings.to(device)\n            \n            optimizer.zero_grad()\n            \n            predictions = model(data, batch_users, batch_items)\n            user_emb = model.user_embedding(batch_users)\n            item_emb = model.item_embedding(batch_items)\n            \n            loss = criterion(predictions, batch_ratings, user_emb, item_emb)\n            loss.backward()\n            \n            # Calculate gradient norm and convert to float\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            total_grad_norm += grad_norm.item() if torch.is_tensor(grad_norm) else grad_norm\n            \n            optimizer.step()\n            total_train_loss += loss.item()\n            \n            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        avg_train_loss = total_train_loss / len(train_loader)\n        avg_grad_norm = total_grad_norm / len(train_loader)\n        \n        # Validation phase\n        model.eval()\n        total_val_loss = 0\n        all_predictions = []\n        all_targets = []\n        \n        with torch.no_grad():\n            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n            for batch_users, batch_items, batch_ratings in val_pbar:\n                batch_users = batch_users.to(device)\n                batch_items = batch_items.to(device)\n                batch_ratings = batch_ratings.to(device)\n                \n                predictions = model(data, batch_users, batch_items)\n                user_emb = model.user_embedding(batch_users)\n                item_emb = model.item_embedding(batch_items)\n                \n                loss = criterion(predictions, batch_ratings, user_emb, item_emb)\n                total_val_loss += loss.item()\n                \n                all_predictions.extend(predictions.cpu().numpy())\n                all_targets.extend(batch_ratings.cpu().numpy())\n                \n                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        avg_val_loss = total_val_loss / len(val_loader)\n        val_rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n        val_mae = mean_absolute_error(all_targets, all_predictions)\n        \n        # Update learning rate\n        scheduler.step(avg_val_loss)\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # Record metrics (ensure all are Python floats)\n        epoch_time = time.time() - epoch_start_time\n        metrics['train_loss'].append(avg_train_loss)\n        metrics['val_loss'].append(avg_val_loss)\n        metrics['val_rmse'].append(val_rmse)\n        metrics['val_mae'].append(val_mae)\n        metrics['learning_rate'].append(current_lr)\n        metrics['epoch_time'].append(epoch_time)\n        metrics['gradient_norm'].append(avg_grad_norm)\n        \n        # Early stopping check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), 'best_model_notebook.pth')\n        else:\n            patience_counter += 1\n        \n        # Print progress\n        if epoch % 5 == 0 or epoch == epochs - 1:\n            print(f'Epoch {epoch+1:03d} | Train Loss: {avg_train_loss:.4f} | '\n                  f'Val Loss: {avg_val_loss:.4f} | Val RMSE: {val_rmse:.4f} | '\n                  f'Val MAE: {val_mae:.4f} | LR: {current_lr:.6f} | Time: {epoch_time:.1f}s')\n        \n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n    \n    # Load best model\n    model.load_state_dict(torch.load('best_model_notebook.pth'))\n    print(\"Training completed!\")\n    \n    return metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training_metrics = train_with_monitoring(\n",
    "    model=model,\n",
    "    data=graph_data,\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    epochs=50,\n",
    "    lr=0.001,\n",
    "    batch_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive training metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "epochs_range = range(1, len(training_metrics['train_loss']) + 1)\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(epochs_range, training_metrics['train_loss'], label='Train Loss', color='blue')\n",
    "axes[0, 0].plot(epochs_range, training_metrics['val_loss'], label='Validation Loss', color='red')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE progression\n",
    "axes[0, 1].plot(epochs_range, training_metrics['val_rmse'], color='green', linewidth=2)\n",
    "axes[0, 1].axhline(y=0.90, color='red', linestyle='--', label='Target RMSE (0.90)')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_title('Validation RMSE Progression')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE progression\n",
    "axes[0, 2].plot(epochs_range, training_metrics['val_mae'], color='orange', linewidth=2)\n",
    "axes[0, 2].axhline(y=0.72, color='red', linestyle='--', label='Target MAE (0.72)')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('MAE')\n",
    "axes[0, 2].set_title('Validation MAE Progression')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "axes[1, 0].plot(epochs_range, training_metrics['learning_rate'], color='purple', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm\n",
    "axes[1, 1].plot(epochs_range, training_metrics['gradient_norm'], color='brown', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Gradient Norm')\n",
    "axes[1, 1].set_title('Gradient Norm Progression')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training time per epoch\n",
    "axes[1, 2].plot(epochs_range, training_metrics['epoch_time'], color='teal', linewidth=2)\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Time (seconds)')\n",
    "axes[1, 2].set_title('Training Time per Epoch')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\n=== Final Training Metrics ===\")\n",
    "print(f\"Final Train Loss: {training_metrics['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {training_metrics['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation RMSE: {training_metrics['val_rmse'][-1]:.4f}\")\n",
    "print(f\"Final Validation MAE: {training_metrics['val_mae'][-1]:.4f}\")\n",
    "print(f\"Total Training Time: {sum(training_metrics['epoch_time']):.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator and perform comprehensive evaluation\n",
    "evaluator = RecommenderEvaluator(model, device=device)\n",
    "\n",
    "print(\"Performing comprehensive evaluation on test set...\")\n",
    "test_results = evaluator.comprehensive_evaluation(graph_data, test_df, k_values=[5, 10, 20])\n",
    "\n",
    "# Display results in a formatted table\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE: {test_results['rmse']:.4f} (Target: ‚â§ 0.90)\")\n",
    "print(f\"MAE: {test_results['mae']:.4f} (Target: ‚â§ 0.72)\")\n",
    "print(\"\\nRanking Metrics:\")\n",
    "for k in [5, 10, 20]:\n",
    "    print(f\"Precision@{k}: {test_results['precision_at_k'][k]:.4f}\")\n",
    "    print(f\"Recall@{k}: {test_results['recall_at_k'][k]:.4f}\")\n",
    "    print(f\"NDCG@{k}: {test_results['ndcg_at_k'][k]:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Check if targets are met\n",
    "target_met = {\n",
    "    'RMSE': test_results['rmse'] <= 0.90,\n",
    "    'MAE': test_results['mae'] <= 0.72,\n",
    "    'Precision@10': test_results['precision_at_k'][10] >= 0.85\n",
    "}\n",
    "\n",
    "print(\"\\n=== TARGET ACHIEVEMENT ===\")\n",
    "for metric, achieved in target_met.items():\n",
    "    status = \"‚úÖ ACHIEVED\" if achieved else \"‚ùå NOT ACHIEVED\"\n",
    "    print(f\"{metric}: {status}\")\n",
    "\n",
    "all_targets_met = all(target_met.values())\n",
    "print(f\"\\nOverall: {'üéâ ALL TARGETS MET!' if all_targets_met else '‚ö†Ô∏è SOME TARGETS NOT MET'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed predictions for analysis\n",
    "rmse, mae, predictions, targets = evaluator.evaluate_ratings(graph_data, test_df)\n",
    "\n",
    "# Create performance analysis plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Prediction vs Actual scatter plot\n",
    "axes[0, 0].scatter(targets, predictions, alpha=0.5, s=10)\n",
    "axes[0, 0].plot([1, 5], [1, 5], 'r--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Actual Rating')\n",
    "axes[0, 0].set_ylabel('Predicted Rating')\n",
    "axes[0, 0].set_title('Predicted vs Actual Ratings')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = predictions - targets\n",
    "axes[0, 1].scatter(targets, residuals, alpha=0.5, s=10)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Actual Rating')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residuals Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "axes[1, 0].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Prediction Error')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Error Distribution')\n",
    "\n",
    "# Performance by rating value\n",
    "rating_performance = {}\n",
    "for rating in range(1, 6):\n",
    "    mask = (targets == rating)\n",
    "    if mask.sum() > 0:\n",
    "        rating_rmse = np.sqrt(mean_squared_error(targets[mask], predictions[mask]))\n",
    "        rating_performance[rating] = rating_rmse\n",
    "\n",
    "ratings = list(rating_performance.keys())\n",
    "rmses = list(rating_performance.values())\n",
    "axes[1, 1].bar(ratings, rmses, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Rating Value')\n",
    "axes[1, 1].set_ylabel('RMSE')\n",
    "axes[1, 1].set_title('RMSE by Rating Value')\n",
    "axes[1, 1].set_xticks(ratings)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\n=== Detailed Performance Statistics ===\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Root Mean Square Error: {rmse:.4f}\")\n",
    "print(f\"Mean Prediction: {np.mean(predictions):.4f}\")\n",
    "print(f\"Mean Actual: {np.mean(targets):.4f}\")\n",
    "print(f\"Prediction Std: {np.std(predictions):.4f}\")\n",
    "print(f\"Actual Std: {np.std(targets):.4f}\")\n",
    "print(f\"Correlation: {np.corrcoef(predictions, targets)[0, 1]:.4f}\")\n",
    "\n",
    "# Error analysis by rating\n",
    "print(\"\\n=== Error Analysis by Rating ===\")\n",
    "for rating in range(1, 6):\n",
    "    mask = (targets == rating)\n",
    "    if mask.sum() > 0:\n",
    "        rating_mae = mean_absolute_error(targets[mask], predictions[mask])\n",
    "        rating_rmse = np.sqrt(mean_squared_error(targets[mask], predictions[mask]))\n",
    "        print(f\"Rating {rating}: MAE={rating_mae:.4f}, RMSE={rating_rmse:.4f}, Count={mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Interpretation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze learned embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get embeddings for a sample of users and items\n",
    "    sample_users = torch.arange(min(100, num_users)).to(device)\n",
    "    sample_items = torch.arange(min(100, num_items)).to(device)\n",
    "    \n",
    "    user_embeddings = model.user_embedding(sample_users).cpu().numpy()\n",
    "    item_embeddings = model.item_embedding(sample_items).cpu().numpy()\n",
    "\n",
    "# Analyze embedding distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# User embedding distribution\n",
    "axes[0, 0].hist(user_embeddings.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_title('User Embedding Distribution')\n",
    "axes[0, 0].set_xlabel('Embedding Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Item embedding distribution\n",
    "axes[0, 1].hist(item_embeddings.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].set_title('Item Embedding Distribution')\n",
    "axes[0, 1].set_xlabel('Embedding Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Embedding similarity heatmap (sample)\n",
    "user_sim = np.corrcoef(user_embeddings[:20])  # Top 20 users\n",
    "im1 = axes[1, 0].imshow(user_sim, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1, 0].set_title('User Embedding Similarity (Sample)')\n",
    "plt.colorbar(im1, ax=axes[1, 0])\n",
    "\n",
    "item_sim = np.corrcoef(item_embeddings[:20])  # Top 20 items\n",
    "im2 = axes[1, 1].imshow(item_sim, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1, 1].set_title('Item Embedding Similarity (Sample)')\n",
    "plt.colorbar(im2, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Embedding Analysis ===\")\n",
    "print(f\"User embedding mean: {np.mean(user_embeddings):.4f}\")\n",
    "print(f\"User embedding std: {np.std(user_embeddings):.4f}\")\n",
    "print(f\"Item embedding mean: {np.mean(item_embeddings):.4f}\")\n",
    "print(f\"Item embedding std: {np.std(item_embeddings):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze recommendation quality for specific users\n",
    "def analyze_user_recommendations(user_id, top_k=10):\n",
    "    # Get user's historical ratings\n",
    "    user_ratings = test_df[test_df['user_id'] == user_id].copy()\n",
    "    \n",
    "    if len(user_ratings) == 0:\n",
    "        print(f\"No test ratings found for user {user_id}\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions for this user's items\n",
    "    user_tensor = torch.LongTensor([user_id] * len(user_ratings)).to(device)\n",
    "    item_tensor = torch.LongTensor(user_ratings['item_id'].values).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data, user_tensor, item_tensor)\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison = pd.DataFrame({\n",
    "        'item_id': user_ratings['item_id'].values,\n",
    "        'actual_rating': user_ratings['rating'].values,\n",
    "        'predicted_rating': predictions,\n",
    "        'error': predictions - user_ratings['rating'].values\n",
    "    })\n",
    "    \n",
    "    comparison = comparison.sort_values('predicted_rating', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== User {user_id} Recommendation Analysis ===\")\n",
    "    print(f\"Total items rated: {len(comparison)}\")\n",
    "    print(f\"Average actual rating: {comparison['actual_rating'].mean():.2f}\")\n",
    "    print(f\"Average predicted rating: {comparison['predicted_rating'].mean():.2f}\")\n",
    "    print(f\"RMSE for this user: {np.sqrt(np.mean(comparison['error']**2)):.4f}\")\n",
    "    \n",
    "    print(f\"\\nTop {top_k} Recommendations:\")\n",
    "    print(comparison.head(top_k).to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Analyze a few random users\n",
    "sample_users = np.random.choice(test_df['user_id'].unique(), size=3, replace=False)\n",
    "for user_id in sample_users:\n",
    "    analyze_user_recommendations(user_id, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ranking Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive ranking metrics visualization\n",
    "k_values = [1, 2, 3, 5, 10, 15, 20]\n",
    "precision_values = [evaluator.precision_at_k(graph_data, test_df, k=k) for k in k_values]\n",
    "recall_values = [evaluator.recall_at_k(graph_data, test_df, k=k) for k in k_values]\n",
    "ndcg_values = [evaluator.ndcg_at_k(graph_data, test_df, k=k) for k in k_values]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Precision@K\n",
    "axes[0, 0].plot(k_values, precision_values, marker='o', linewidth=2, markersize=8)\n",
    "axes[0, 0].axhline(y=0.85, color='red', linestyle='--', label='Target (0.85)')\n",
    "axes[0, 0].set_xlabel('K')\n",
    "axes[0, 0].set_ylabel('Precision@K')\n",
    "axes[0, 0].set_title('Precision@K Performance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Recall@K\n",
    "axes[0, 1].plot(k_values, recall_values, marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[0, 1].set_xlabel('K')\n",
    "axes[0, 1].set_ylabel('Recall@K')\n",
    "axes[0, 1].set_title('Recall@K Performance')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# NDCG@K\n",
    "axes[1, 0].plot(k_values, ndcg_values, marker='^', linewidth=2, markersize=8, color='orange')\n",
    "axes[1, 0].set_xlabel('K')\n",
    "axes[1, 0].set_ylabel('NDCG@K')\n",
    "axes[1, 0].set_title('NDCG@K Performance')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Combined metrics\n",
    "axes[1, 1].plot(k_values, precision_values, marker='o', label='Precision@K', linewidth=2)\n",
    "axes[1, 1].plot(k_values, recall_values, marker='s', label='Recall@K', linewidth=2)\n",
    "axes[1, 1].plot(k_values, ndcg_values, marker='^', label='NDCG@K', linewidth=2)\n",
    "axes[1, 1].set_xlabel('K')\n",
    "axes[1, 1].set_ylabel('Metric Value')\n",
    "axes[1, 1].set_title('All Ranking Metrics')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed ranking results\n",
    "print(\"\\n=== Detailed Ranking Metrics ===\")\n",
    "ranking_df = pd.DataFrame({\n",
    "    'K': k_values,\n",
    "    'Precision@K': precision_values,\n",
    "    'Recall@K': recall_values,\n",
    "    'NDCG@K': ndcg_values\n",
    "})\n",
    "print(ranking_df.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Target vs Achieved comparison\n",
    "metrics = ['RMSE', 'MAE', 'Precision@10']\n",
    "targets = [0.90, 0.72, 0.85]\n",
    "achieved = [test_results['rmse'], test_results['mae'], test_results['precision_at_k'][10]]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, targets, width, label='Target', alpha=0.7, color='lightcoral')\n",
    "bars2 = axes[0].bar(x + width/2, achieved, width, label='Achieved', alpha=0.7, color='lightblue')\n",
    "\n",
    "axes[0].set_xlabel('Metrics')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].set_title('Target vs Achieved Performance')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_value_labels(ax, bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "add_value_labels(axes[0], bars1)\n",
    "add_value_labels(axes[0], bars2)\n",
    "\n",
    "# Model performance summary\n",
    "performance_summary = {\n",
    "    'Metric': ['RMSE', 'MAE', 'Precision@5', 'Precision@10', 'Precision@20', 'NDCG@10'],\n",
    "    'Value': [\n",
    "        test_results['rmse'],\n",
    "        test_results['mae'],\n",
    "        test_results['precision_at_k'][5],\n",
    "        test_results['precision_at_k'][10],\n",
    "        test_results['precision_at_k'][20],\n",
    "        test_results['ndcg_at_k'][10]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a table-like visualization\n",
    "axes[1].axis('tight')\n",
    "axes[1].axis('off')\n",
    "table = axes[1].table(cellText=[[f\"{v:.4f}\"] for v in performance_summary['Value']],\n",
    "                      rowLabels=performance_summary['Metric'],\n",
    "                      colLabels=['Score'],\n",
    "                      cellLoc='center',\n",
    "                      loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.2, 1.5)\n",
    "axes[1].set_title('Final Model Performance Summary', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY - GNN-BASED RECOMMENDATION SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüéØ TARGET ACHIEVEMENTS:\")\n",
    "print(f\"   RMSE: {test_results['rmse']:.4f} (Target: ‚â§ 0.90) {'‚úÖ' if test_results['rmse'] <= 0.90 else '‚ùå'}\")\n",
    "print(f\"   MAE: {test_results['mae']:.4f} (Target: ‚â§ 0.72) {'‚úÖ' if test_results['mae'] <= 0.72 else '‚ùå'}\")\n",
    "print(f\"   Precision@10: {test_results['precision_at_k'][10]:.4f} (Target: ‚â• 0.85) {'‚úÖ' if test_results['precision_at_k'][10] >= 0.85 else '‚ùå'}\")\n",
    "\n",
    "print(f\"\\nüìä ADDITIONAL METRICS:\")\n",
    "print(f\"   Precision@5: {test_results['precision_at_k'][5]:.4f}\")\n",
    "print(f\"   Precision@20: {test_results['precision_at_k'][20]:.4f}\")\n",
    "print(f\"   Recall@10: {test_results['recall_at_k'][10]:.4f}\")\n",
    "print(f\"   NDCG@10: {test_results['ndcg_at_k'][10]:.4f}\")\n",
    "\n",
    "print(f\"\\nüîß MODEL ARCHITECTURE:\")\n",
    "print(f\"   Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Embedding Dimension: 64\")\n",
    "print(f\"   Hidden Dimension: 128\")\n",
    "print(f\"   GNN Layers: 3\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "print(f\"\\nüìà TRAINING STATISTICS:\")\n",
    "print(f\"   Final Training Loss: {training_metrics['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final Validation Loss: {training_metrics['val_loss'][-1]:.4f}\")\n",
    "print(f\"   Total Training Time: {sum(training_metrics['epoch_time']):.1f} seconds\")\n",
    "print(f\"   Epochs Completed: {len(training_metrics['train_loss'])}\")\n",
    "\n",
    "print(f\"\\nüéâ SYSTEM PERFORMANCE: {'EXCELLENT' if all_targets_met else 'GOOD'}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}